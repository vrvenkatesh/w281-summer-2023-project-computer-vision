{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fruit and Vegetable Classification\n",
    "## \\# Class activation heatmap for image classification\n",
    "## \\# Grad-CAM class activation visualization\n",
    "\n",
    "Having 3861 images of 36 different fruits/vegetables\n",
    "\n",
    "![fruit vegetable](https://i.imgur.com/KUAcIQD.jpeg)\n",
    "\n",
    "![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)\n",
    "(https://colab.research.google.com/github/MichaelTay/w266-spring2023-final/blob/main/Project.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Table of contents</h1>\n",
    " \n",
    "\n",
    "<ul>\n",
    "<li><a href=\"#1\"><strong>1. Loading and preprocessing</strong></a>\n",
    "</ul>\n",
    "    \n",
    "<ul>\n",
    "<li><a href=\"#2\"><strong>2. Load the Images with a generator and Data Augmentation</strong></a>\n",
    "</ul>\n",
    "\n",
    "<ul>\n",
    "<li><a href=\"#3\"><strong>3. Train the model</strong></a>\n",
    "</ul>\n",
    "\n",
    "<ul>\n",
    "<li><a href=\"#4\"><strong>4. Visualize the result</strong></a>\n",
    "</ul>\n",
    "\n",
    "<ul>\n",
    "<li><a href=\"#5\"><strong>5. Class activation heatmap for image classification</strong></a>\n",
    "</ul>\n",
    "\n",
    "# Context\n",
    "\n",
    "This dataset contains images of the following food items:\n",
    "\n",
    "- **fruits**: banana, apple, pear, grapes, orange, kiwi, watermelon, pomegranate, pineapple, mango\n",
    "- **vegetables**: cucumber, carrot, capsicum, onion, potato, lemon, tomato, raddish, beetroot, cabbage, lettuce, spinach, soy bean, cauliflower, bell pepper, chilli pepper, turnip, corn, sweetcorn, sweet potato, paprika, jalepeño, ginger, garlic, peas, eggplant\n",
    "\n",
    "# Content\n",
    "This dataset contains three folders:\n",
    "\n",
    "- train (100 images each)\n",
    "- test (10 images each)\n",
    "- validation (10 images each)\n",
    "each of the above folders contains subfolders for different fruits and vegetables wherein the images for respective food items are present# Context\n",
    "\n",
    "This dataset contains images of the following food items:\n",
    "\n",
    "- **fruits**: banana, apple, pear, grapes, orange, kiwi, watermelon, pomegranate, pineapple, mango\n",
    "- **vegetables**: cucumber, carrot, capsicum, onion, potato, lemon, tomato, raddish, beetroot, cabbage, lettuce, spinach, soy bean, cauliflower, bell pepper, chilli pepper, turnip, corn, sweetcorn, sweet potato, paprika, jalepeño, ginger, garlic, peas, eggplant\n",
    "\n",
    "# Content\n",
    "This dataset contains three folders:\n",
    "\n",
    "- train (100 images each)\n",
    "- test (10 images each)\n",
    "- validation (10 images each)\n",
    "each of the above folders contains subfolders for different fruits and vegetables wherein the images for respective food items are present\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading and preprocessing<a class=\"anchor\" id=\"1\"></a><a class=\"anchor\" id=\"1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load the Drive helper and mount\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# This will prompt for authorization.\u001b[39;00m\n\u001b[1;32m      7\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Load the Drive helper and mount\n",
    "from google.colab import drive\n",
    "\n",
    "# This will prompt for authorization.\n",
    "drive.mount('/content/drive')\n",
    "rootdir = '/content/drive/MyDrive'\n",
    "\n",
    "# Replace your folder here\n",
    "w281_directory = '/Berkeley/w281/Fruits-and-Vegetable-Classification/input'\n",
    "filepath = rootdir + w281_directory\n",
    "files_path = [os.path.abspath(x) for x in os.listdir(filepath)]\n",
    "print(files_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-30T08:40:40.436565Z",
     "iopub.status.busy": "2021-05-30T08:40:40.436021Z",
     "iopub.status.idle": "2021-05-30T08:40:42.259382Z",
     "shell.execute_reply": "2021-05-30T08:40:42.258422Z",
     "shell.execute_reply.started": "2021-05-30T08:40:40.436446Z"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Traceback (most recent call last):\n  File \"/Users/michaeltay/miniforge3/lib/python3.9/site-packages/tensorflow/python/__init__.py\", line 61, in <module>\n    from tensorflow.python import pywrap_tensorflow\n  File \"/Users/michaeltay/miniforge3/lib/python3.9/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 28, in <module>\n    _pywrap_tensorflow = swig_import_helper()\n  File \"/Users/michaeltay/miniforge3/lib/python3.9/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)\n  File \"/Users/michaeltay/miniforge3/lib/python3.9/imp.py\", line 242, in load_module\n    return load_dynamic(name, filename, file)\n  File \"/Users/michaeltay/miniforge3/lib/python3.9/imp.py\", line 342, in load_dynamic\n    return _load(spec)\nImportError: dlopen(/Users/michaeltay/miniforge3/lib/python3.9/site-packages/tensorflow/python/_pywrap_tensorflow.so, 0x000A): tried: '/Users/michaeltay/miniforge3/lib/python3.9/site-packages/tensorflow/python/_pywrap_tensorflow.so' (mach-o file, but is an incompatible architecture (have 'x86_64', need 'arm64')), '/System/Volumes/Preboot/Cryptexes/OS/Users/michaeltay/miniforge3/lib/python3.9/site-packages/tensorflow/python/_pywrap_tensorflow.so' (no such file), '/Users/michaeltay/miniforge3/lib/python3.9/site-packages/tensorflow/python/_pywrap_tensorflow.so' (mach-o file, but is an incompatible architecture (have 'x86_64', need 'arm64'))\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#import_error\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/tensorflow/python/__init__.py:61\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     60\u001b[0m sys\u001b[38;5;241m.\u001b[39msetdlopenflags(_default_dlopen_flags \u001b[38;5;241m|\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mRTLD_GLOBAL)\n\u001b[0;32m---> 61\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow\n\u001b[1;32m     62\u001b[0m sys\u001b[38;5;241m.\u001b[39msetdlopenflags(_default_dlopen_flags)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/tensorflow/python/pywrap_tensorflow.py:28\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _mod\n\u001b[0;32m---> 28\u001b[0m _pywrap_tensorflow \u001b[38;5;241m=\u001b[39m \u001b[43mswig_import_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m swig_import_helper\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/tensorflow/python/pywrap_tensorflow.py:24\u001b[0m, in \u001b[0;36mswig_import_helper\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 24\u001b[0m     _mod \u001b[38;5;241m=\u001b[39m \u001b[43mimp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_pywrap_tensorflow\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpathname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/imp.py:242\u001b[0m, in \u001b[0;36mload_module\u001b[0;34m(name, file, filename, details)\u001b[0m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 242\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_dynamic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m type_ \u001b[38;5;241m==\u001b[39m PKG_DIRECTORY:\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/imp.py:342\u001b[0m, in \u001b[0;36mload_dynamic\u001b[0;34m(name, path, file)\u001b[0m\n\u001b[1;32m    340\u001b[0m spec \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mmachinery\u001b[38;5;241m.\u001b[39mModuleSpec(\n\u001b[1;32m    341\u001b[0m     name\u001b[38;5;241m=\u001b[39mname, loader\u001b[38;5;241m=\u001b[39mloader, origin\u001b[38;5;241m=\u001b[39mpath)\n\u001b[0;32m--> 342\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspec\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mImportError\u001b[0m: dlopen(/Users/michaeltay/miniforge3/lib/python3.9/site-packages/tensorflow/python/_pywrap_tensorflow.so, 0x000A): tried: '/Users/michaeltay/miniforge3/lib/python3.9/site-packages/tensorflow/python/_pywrap_tensorflow.so' (mach-o file, but is an incompatible architecture (have 'x86_64', need 'arm64')), '/System/Volumes/Preboot/Cryptexes/OS/Users/michaeltay/miniforge3/lib/python3.9/site-packages/tensorflow/python/_pywrap_tensorflow.so' (no such file), '/Users/michaeltay/miniforge3/lib/python3.9/site-packages/tensorflow/python/_pywrap_tensorflow.so' (mach-o file, but is an incompatible architecture (have 'x86_64', need 'arm64'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpath\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Create a list with the filepaths for training and testing\u001b[39;00m\n\u001b[1;32m      9\u001b[0m train_dir \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./input/train\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/tensorflow/__init__.py:24\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m print_function\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# pylint: disable=wildcard-import\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# pylint: enable=wildcard-import\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Lazily import the `tf.contrib` module. This avoids loading all of the\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# dependencies of `tf.contrib` at `import tensorflow` time.\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01m_LazyContribLoader\u001b[39;00m(\u001b[38;5;28mobject\u001b[39m):\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/tensorflow/python/__init__.py:72\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m     68\u001b[0m   msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFailed to load the native TensorFlow runtime.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;124mSee https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#import_error\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124mfor some common reasons and solutions.  Include the entire stack trace\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124mabove this error message when asking for help.\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m \u001b[38;5;241m%\u001b[39m traceback\u001b[38;5;241m.\u001b[39mformat_exc()\n\u001b[0;32m---> 72\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Protocol buffers\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph_pb2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "\u001b[0;31mImportError\u001b[0m: Traceback (most recent call last):\n  File \"/Users/michaeltay/miniforge3/lib/python3.9/site-packages/tensorflow/python/__init__.py\", line 61, in <module>\n    from tensorflow.python import pywrap_tensorflow\n  File \"/Users/michaeltay/miniforge3/lib/python3.9/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 28, in <module>\n    _pywrap_tensorflow = swig_import_helper()\n  File \"/Users/michaeltay/miniforge3/lib/python3.9/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)\n  File \"/Users/michaeltay/miniforge3/lib/python3.9/imp.py\", line 242, in load_module\n    return load_dynamic(name, filename, file)\n  File \"/Users/michaeltay/miniforge3/lib/python3.9/imp.py\", line 342, in load_dynamic\n    return _load(spec)\nImportError: dlopen(/Users/michaeltay/miniforge3/lib/python3.9/site-packages/tensorflow/python/_pywrap_tensorflow.so, 0x000A): tried: '/Users/michaeltay/miniforge3/lib/python3.9/site-packages/tensorflow/python/_pywrap_tensorflow.so' (mach-o file, but is an incompatible architecture (have 'x86_64', need 'arm64')), '/System/Volumes/Preboot/Cryptexes/OS/Users/michaeltay/miniforge3/lib/python3.9/site-packages/tensorflow/python/_pywrap_tensorflow.so' (no such file), '/Users/michaeltay/miniforge3/lib/python3.9/site-packages/tensorflow/python/_pywrap_tensorflow.so' (mach-o file, but is an incompatible architecture (have 'x86_64', need 'arm64'))\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#import_error\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os.path\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "# Create a list with the filepaths for training and testing\n",
    "train_dir = Path('./input/train')\n",
    "train_filepaths = list(train_dir.glob(r'**/*.jpg'))\n",
    "\n",
    "test_dir = Path('./input/test')\n",
    "test_filepaths = list(test_dir.glob(r'**/*.jpg'))\n",
    "\n",
    "val_dir = Path('./input/validation')\n",
    "val_filepaths = list(test_dir.glob(r'**/*.jpg'))\n",
    "\n",
    "def proc_img(filepath):\n",
    "    \"\"\" Create a DataFrame with the filepath and the labels of the pictures\n",
    "    \"\"\"\n",
    "\n",
    "    labels = [str(filepath[i]).split(\"/\")[-2] \\\n",
    "              for i in range(len(filepath))]\n",
    "\n",
    "    filepath = pd.Series(filepath, name='Filepath').astype(str)\n",
    "    labels = pd.Series(labels, name='Label')\n",
    "\n",
    "    # Concatenate filepaths and labels\n",
    "    df = pd.concat([filepath, labels], axis=1)\n",
    "\n",
    "    # Shuffle the DataFrame and reset index\n",
    "    df = df.sample(frac=1).reset_index(drop = True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "train_df = proc_img(train_filepaths)\n",
    "test_df = proc_img(test_filepaths)\n",
    "val_df = proc_img(val_filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-30T08:40:42.261208Z",
     "iopub.status.busy": "2021-05-30T08:40:42.260878Z",
     "iopub.status.idle": "2021-05-30T08:40:42.269274Z",
     "shell.execute_reply": "2021-05-30T08:40:42.268398Z",
     "shell.execute_reply.started": "2021-05-30T08:40:42.261172Z"
    }
   },
   "outputs": [],
   "source": [
    "print('-- Training set --\\n')\n",
    "print(f'Number of pictures: {train_df.shape[0]}\\n')\n",
    "print(f'Number of different labels: {len(train_df.Label.unique())}\\n')\n",
    "print(f'Labels: {train_df.Label.unique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-30T08:40:42.271618Z",
     "iopub.status.busy": "2021-05-30T08:40:42.271099Z",
     "iopub.status.idle": "2021-05-30T08:40:42.291191Z",
     "shell.execute_reply": "2021-05-30T08:40:42.290395Z",
     "shell.execute_reply.started": "2021-05-30T08:40:42.271581Z"
    }
   },
   "outputs": [],
   "source": [
    "# The DataFrame with the filepaths in one column and the labels in the other one\n",
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-30T08:40:42.293366Z",
     "iopub.status.busy": "2021-05-30T08:40:42.292982Z",
     "iopub.status.idle": "2021-05-30T08:40:48.718339Z",
     "shell.execute_reply": "2021-05-30T08:40:48.717443Z",
     "shell.execute_reply.started": "2021-05-30T08:40:42.293327Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame with one Label of each category\n",
    "df_unique = train_df.copy().drop_duplicates(subset=[\"Label\"]).reset_index()\n",
    "\n",
    "# Display some pictures of the dataset\n",
    "fig, axes = plt.subplots(nrows=6, ncols=6, figsize=(8, 7),\n",
    "                        subplot_kw={'xticks': [], 'yticks': []})\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(plt.imread(df_unique.Filepath[i]))\n",
    "    ax.set_title(df_unique.Label[i], fontsize = 12)\n",
    "plt.tight_layout(pad=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load the Images with a generator and Data Augmentation<a class=\"anchor\" id=\"2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-30T08:40:48.719985Z",
     "iopub.status.busy": "2021-05-30T08:40:48.719601Z",
     "iopub.status.idle": "2021-05-30T08:40:50.072288Z",
     "shell.execute_reply": "2021-05-30T08:40:50.071197Z",
     "shell.execute_reply.started": "2021-05-30T08:40:48.719949Z"
    }
   },
   "outputs": [],
   "source": [
    "train_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input\n",
    ")\n",
    "\n",
    "test_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input\n",
    ")\n",
    "\n",
    "train_images = train_generator.flow_from_dataframe(\n",
    "    dataframe=train_df,\n",
    "    x_col='Filepath',\n",
    "    y_col='Label',\n",
    "    target_size=(224, 224),\n",
    "    color_mode='rgb',\n",
    "    class_mode='categorical',\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    seed=0,\n",
    "    rotation_range=30,\n",
    "    zoom_range=0.15,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.15,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode=\"nearest\"\n",
    ")\n",
    "\n",
    "val_images = train_generator.flow_from_dataframe(\n",
    "    dataframe=val_df,\n",
    "    x_col='Filepath',\n",
    "    y_col='Label',\n",
    "    target_size=(224, 224),\n",
    "    color_mode='rgb',\n",
    "    class_mode='categorical',\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    seed=0,\n",
    "    rotation_range=30,\n",
    "    zoom_range=0.15,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.15,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode=\"nearest\"\n",
    ")\n",
    "\n",
    "test_images = test_generator.flow_from_dataframe(\n",
    "    dataframe=test_df,\n",
    "    x_col='Filepath',\n",
    "    y_col='Label',\n",
    "    target_size=(224, 224),\n",
    "    color_mode='rgb',\n",
    "    class_mode='categorical',\n",
    "    batch_size=32,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2021-05-30T08:40:50.075611Z",
     "iopub.status.busy": "2021-05-30T08:40:50.075326Z",
     "iopub.status.idle": "2021-05-30T08:40:52.071504Z",
     "shell.execute_reply": "2021-05-30T08:40:52.070569Z",
     "shell.execute_reply.started": "2021-05-30T08:40:50.075582Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the pretained model\n",
    "pretrained_model = tf.keras.applications.MobileNetV2(\n",
    "    input_shape=(224, 224, 3),\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    pooling='avg'\n",
    ")\n",
    "pretrained_model.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train the model<a class=\"anchor\" id=\"3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2021-05-30T08:40:52.07325Z",
     "iopub.status.busy": "2021-05-30T08:40:52.072882Z",
     "iopub.status.idle": "2021-05-30T08:51:35.585279Z",
     "shell.execute_reply": "2021-05-30T08:51:35.58375Z",
     "shell.execute_reply.started": "2021-05-30T08:40:52.073197Z"
    }
   },
   "outputs": [],
   "source": [
    "inputs = pretrained_model.input\n",
    "\n",
    "x = tf.keras.layers.Dense(128, activation='relu')(pretrained_model.output)\n",
    "x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "\n",
    "outputs = tf.keras.layers.Dense(36, activation='softmax')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_images,\n",
    "    validation_data=val_images,\n",
    "    batch_size = 32,\n",
    "    epochs=5,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=2,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-30T08:51:35.593715Z",
     "iopub.status.busy": "2021-05-30T08:51:35.5933Z",
     "iopub.status.idle": "2021-05-30T08:51:35.795823Z",
     "shell.execute_reply": "2021-05-30T08:51:35.795014Z",
     "shell.execute_reply.started": "2021-05-30T08:51:35.593665Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history)[['accuracy','val_accuracy']].plot()\n",
    "plt.title(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-30T08:51:35.797874Z",
     "iopub.status.busy": "2021-05-30T08:51:35.797473Z",
     "iopub.status.idle": "2021-05-30T08:51:35.969543Z",
     "shell.execute_reply": "2021-05-30T08:51:35.968488Z",
     "shell.execute_reply.started": "2021-05-30T08:51:35.797833Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history)[['loss','val_loss']].plot()\n",
    "plt.title(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Visualize the result<a class=\"anchor\" id=\"4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2021-05-30T08:51:35.971442Z",
     "iopub.status.busy": "2021-05-30T08:51:35.971059Z",
     "iopub.status.idle": "2021-05-30T08:51:53.343485Z",
     "shell.execute_reply": "2021-05-30T08:51:53.342485Z",
     "shell.execute_reply.started": "2021-05-30T08:51:35.971403Z"
    }
   },
   "outputs": [],
   "source": [
    "# Predict the label of the test_images\n",
    "pred = model.predict(test_images)\n",
    "pred = np.argmax(pred,axis=1)\n",
    "\n",
    "# Map the label\n",
    "labels = (train_images.class_indices)\n",
    "labels = dict((v,k) for k,v in labels.items())\n",
    "pred = [labels[k] for k in pred]\n",
    "\n",
    "y_test = [labels[k] for k in test_images.classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-30T08:54:38.504565Z",
     "iopub.status.busy": "2021-05-30T08:54:38.504182Z",
     "iopub.status.idle": "2021-05-30T08:54:38.514042Z",
     "shell.execute_reply": "2021-05-30T08:54:38.510923Z",
     "shell.execute_reply.started": "2021-05-30T08:54:38.504532Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "acc = accuracy_score(y_test, pred)\n",
    "print(f'Accuracy on the test set: {100*acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-30T08:51:53.345398Z",
     "iopub.status.busy": "2021-05-30T08:51:53.345004Z",
     "iopub.status.idle": "2021-05-30T08:51:57.833613Z",
     "shell.execute_reply": "2021-05-30T08:51:57.832791Z",
     "shell.execute_reply.started": "2021-05-30T08:51:53.345345Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "cf_matrix = confusion_matrix(y_test, pred, normalize='true')\n",
    "plt.figure(figsize = (15,10))\n",
    "sns.heatmap(cf_matrix, \n",
    "            annot=True, \n",
    "            xticklabels = sorted(set(y_test)), \n",
    "            yticklabels = sorted(set(y_test)),\n",
    "            )\n",
    "plt.title('Normalized Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-30T08:51:57.835179Z",
     "iopub.status.busy": "2021-05-30T08:51:57.834852Z",
     "iopub.status.idle": "2021-05-30T08:51:59.808507Z",
     "shell.execute_reply": "2021-05-30T08:51:59.807411Z",
     "shell.execute_reply.started": "2021-05-30T08:51:57.835151Z"
    }
   },
   "outputs": [],
   "source": [
    "# Display some pictures of the dataset with their labels and the predictions\n",
    "fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(15, 15),\n",
    "                        subplot_kw={'xticks': [], 'yticks': []})\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(plt.imread(test_df.Filepath.iloc[i]))\n",
    "    ax.set_title(f\"True: {test_df.Label.iloc[i]}\\nPredicted: {pred[i]}\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Class activation heatmap for image classification<a class=\"anchor\" id=\"5\"></a>\n",
    "### Grad-CAM class activation visualization\n",
    "*Code adapted from keras.io*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-30T08:51:59.810749Z",
     "iopub.status.busy": "2021-05-30T08:51:59.810028Z",
     "iopub.status.idle": "2021-05-30T08:51:59.829867Z",
     "shell.execute_reply": "2021-05-30T08:51:59.828894Z",
     "shell.execute_reply.started": "2021-05-30T08:51:59.810696Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "\n",
    "def get_img_array(img_path, size):\n",
    "    img = tf.keras.preprocessing.image.load_img(img_path, target_size=size)\n",
    "    array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "    # We add a dimension to transform our array into a \"batch\"\n",
    "    # of size \"size\"\n",
    "    array = np.expand_dims(array, axis=0)\n",
    "    return array\n",
    "\n",
    "def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n",
    "    # First, we create a model that maps the input image to the activations\n",
    "    # of the last conv layer as well as the output predictions\n",
    "    grad_model = tf.keras.models.Model(\n",
    "        [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]\n",
    "    )\n",
    "\n",
    "    # Then, we compute the gradient of the top predicted class for our input image\n",
    "    # with respect to the activations of the last conv layer\n",
    "    with tf.GradientTape() as tape:\n",
    "        last_conv_layer_output, preds = grad_model(img_array)\n",
    "        if pred_index is None:\n",
    "            pred_index = tf.argmax(preds[0])\n",
    "        class_channel = preds[:, pred_index]\n",
    "\n",
    "    # This is the gradient of the output neuron (top predicted or chosen)\n",
    "    # with regard to the output feature map of the last conv layer\n",
    "    grads = tape.gradient(class_channel, last_conv_layer_output)\n",
    "\n",
    "    # This is a vector where each entry is the mean intensity of the gradient\n",
    "    # over a specific feature map channel\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "\n",
    "    # We multiply each channel in the feature map array\n",
    "    # by \"how important this channel is\" with regard to the top predicted class\n",
    "    # then sum all the channels to obtain the heatmap class activation\n",
    "    last_conv_layer_output = last_conv_layer_output[0]\n",
    "    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n",
    "    heatmap = tf.squeeze(heatmap)\n",
    "\n",
    "    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n",
    "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
    "    return heatmap.numpy()\n",
    "\n",
    "def save_and_display_gradcam(img_path, heatmap, cam_path=\"cam.jpg\", alpha=0.4):\n",
    "    # Load the original image\n",
    "    img = tf.keras.preprocessing.image.load_img(img_path)\n",
    "    img = tf.keras.preprocessing.image.img_to_array(img)\n",
    "\n",
    "    # Rescale heatmap to a range 0-255\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "\n",
    "    # Use jet colormap to colorize heatmap\n",
    "    jet = cm.get_cmap(\"jet\")\n",
    "\n",
    "    # Use RGB values of the colormap\n",
    "    jet_colors = jet(np.arange(256))[:, :3]\n",
    "    jet_heatmap = jet_colors[heatmap]\n",
    "\n",
    "    # Create an image with RGB colorized heatmap\n",
    "    jet_heatmap = tf.keras.preprocessing.image.array_to_img(jet_heatmap)\n",
    "    jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\n",
    "    jet_heatmap = tf.keras.preprocessing.image.img_to_array(jet_heatmap)\n",
    "\n",
    "    # Superimpose the heatmap on original image\n",
    "    superimposed_img = jet_heatmap * alpha + img\n",
    "    superimposed_img = tf.keras.preprocessing.image.array_to_img(superimposed_img)\n",
    "\n",
    "    # Save the superimposed image\n",
    "    superimposed_img.save(cam_path)\n",
    "\n",
    "    # Display Grad CAM\n",
    "#     display(Image(cam_path))\n",
    "    \n",
    "    return cam_path\n",
    "    \n",
    "preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input\n",
    "decode_predictions = tf.keras.applications.mobilenet_v2.decode_predictions\n",
    "\n",
    "last_conv_layer_name = \"Conv_1\"\n",
    "img_size = (224,224)\n",
    "\n",
    "# Remove last layer's softmax\n",
    "model.layers[-1].ativation = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-30T08:51:59.832075Z",
     "iopub.status.busy": "2021-05-30T08:51:59.83139Z",
     "iopub.status.idle": "2021-05-30T08:52:04.070361Z",
     "shell.execute_reply": "2021-05-30T08:52:04.069257Z",
     "shell.execute_reply.started": "2021-05-30T08:51:59.832037Z"
    }
   },
   "outputs": [],
   "source": [
    "# Display the part of the pictures used by the neural network to classify the pictures\n",
    "fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(15, 15),\n",
    "                        subplot_kw={'xticks': [], 'yticks': []})\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    img_path = test_df.Filepath.iloc[i]\n",
    "    img_array = preprocess_input(get_img_array(img_path, size=img_size))\n",
    "    heatmap = make_gradcam_heatmap(img_array, model, last_conv_layer_name)\n",
    "    cam_path = save_and_display_gradcam(img_path, heatmap)\n",
    "    ax.imshow(plt.imread(cam_path))\n",
    "    ax.set_title(f\"True: {test_df.Label.iloc[i]}\\nPredicted: {pred[i]}\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
